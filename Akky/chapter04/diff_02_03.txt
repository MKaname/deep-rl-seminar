2c2,4
< import gym, gym.spaces
---
> import random
> import gym
> import gym.spaces
13,14c15,17
< BATCH_SIZE = 16
< PERCENTILE = 70
---
> BATCH_SIZE = 100
> PERCENTILE = 30
> GAMMA = 0.9
72,74c75,76
<     rewards = list(map(lambda s: s.reward, batch))
<     reward_bound = np.percentile(rewards, percentile)
<     reward_mean = float(np.mean(rewards))
---
>     disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))
>     reward_bound = np.percentile(disc_rewards, percentile)
78,82c80,85
<     for example in batch:
<         if example.reward < reward_bound:
<             continue
<         train_obs.extend(map(lambda step: step.observation, example.steps))
<         train_act.extend(map(lambda step: step.action, example.steps))
---
>     elite_batch = []
>     for example, discounted_reward in zip(batch, disc_rewards):
>         if discounted_reward > reward_bound:
>             train_obs.extend(map(lambda step: step.observation, example.steps))
>             train_act.extend(map(lambda step: step.action, example.steps))
>             elite_batch.append(example)
84,86c87
<     train_obs_v = torch.FloatTensor(train_obs)
<     train_act_v = torch.LongTensor(train_act)
<     return train_obs_v, train_act_v, reward_bound, reward_mean
---
>     return elite_batch, train_obs, train_act, reward_bound
89a91
>     random.seed(12345)
97,98c99,100
<     optimizer = optim.Adam(params=net.parameters(), lr=0.01)
<     writer = SummaryWriter(comment="-frozenlake-naive")
---
>     optimizer = optim.Adam(params=net.parameters(), lr=0.001)
>     writer = SummaryWriter(comment="-frozenlake-tweaked")
99a102
>     full_batch = []
101c104,111
<         obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)
---
>         reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))
>         full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)
>         if not full_batch:
>             continue
>         obs_v = torch.FloatTensor(obs)
>         acts_v = torch.LongTensor(acts)
>         full_batch = full_batch[-500:]
> 
107,108c117,118
<         print("%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f" % (
<             iter_no, loss_v.item(), reward_m, reward_b))
---
>         print("%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d" % (
>             iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))
110,112c120,122
<         writer.add_scalar("reward_bound", reward_b, iter_no)
<         writer.add_scalar("reward_mean", reward_m, iter_no)
<         if reward_m > 0.8:
---
>         writer.add_scalar("reward_mean", reward_mean, iter_no)
>         writer.add_scalar("reward_bound", reward_bound, iter_no)
>         if reward_mean > 0.8:
